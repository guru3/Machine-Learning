{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use lingspam corpus for our mail spam filtering.\n",
    "#### Let's start with function to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "\n",
    "def get_directory_path( set_lemmatizer : bool, use_stop_list: bool ):\n",
    "    base_path = './lingspam_public/'\n",
    "    directory_map = { (False, False):'bare',(True, False):'lemm',\n",
    "              (True, True):'lemm_stop',(False,True):'stop' };\n",
    "    directory = directory_map[(set_lemmatizer, use_stop_list)];\n",
    "    return base_path + directory;\n",
    "\n",
    "def mail_content_to_word_freq_map( email_body : str ):\n",
    "    mail_content_to_word_f = {}\n",
    "    for word in email_body.split(' '):\n",
    "        if not ( word in mail_content_to_word_f.keys() ):\n",
    "            mail_content_to_word_f[ word ] = 0;\n",
    "        mail_content_to_word_f[ word ] = mail_content_to_word_f[ word ] + 1\n",
    "    return mail_content_to_word_f;\n",
    "\n",
    "def get_datasets( set_lemmatizer : bool, use_stop_list : bool, include_subject=False ):\n",
    "    dirIs = get_directory_path( set_lemmatizer, use_stop_list );\n",
    "    datasets = [];\n",
    "    regex = re.compile('[^a-zA-Z ]')\n",
    "    for i in range(1,11):\n",
    "        mailsToIsSpam = {}\n",
    "        tempDir = dirIs + '/part'+str(i)+'/';\n",
    "        for r, d, f in os.walk(tempDir):\n",
    "            for file in f:\n",
    "                isSpam = False;\n",
    "                if( 'spmsg' in file ):\n",
    "                    isSpam = True;\n",
    "                fileData = open(tempDir +file,'r').read()\n",
    "                re.sub(\"[^a-zA-Z ]\",' ', fileData);\n",
    "                re.sub(\"[ ]+\", ' ', fileData);\n",
    "                if not include_subject:\n",
    "                    fileData = '\\n'.join(fileData.split('\\n')[2:])\n",
    "                mailsToIsSpam[ fileData ] = isSpam;\n",
    "        datasets.append( mailsToIsSpam );\n",
    "    return datasets;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question is do we need to weed out single characters like *, \\n, etc that get into our input before we train/test our learning algorithm ? Those words may be occuring with very very low probability, so let's try it out without that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assumption that 10% of emails we receive are spam\n",
    "prob_email_is_spam = 0.1; #10 percent of mails\n",
    "prob_email_is_non_spam = 1 - prob_email_is_spam;\n",
    "spam = 'spam'\n",
    "notspam = 'nspam'\n",
    "\n",
    "def train( training_data_list ):\n",
    "    #we need to learn P( word = w | mail = spam )\n",
    "    category_to_word_to_count = {spam:{}, notspam:{}};\n",
    "    category_count = {spam:0, notspam:0};\n",
    "    word_map = {};\n",
    "    for training_data in training_data_list:\n",
    "        for data in training_data.keys():\n",
    "            isSpam = training_data[ data ]\n",
    "            freq_map = mail_content_to_word_freq_map( data )\n",
    "            total_words = sum( freq_map.values() )\n",
    "            if isSpam:\n",
    "                category = 'spam'\n",
    "            else:\n",
    "                category = 'nspam'\n",
    "            category_count[category] = category_count[category] + 1;\n",
    "            for word in freq_map.keys():\n",
    "                if( not( word in category_to_word_to_count[category]) ):\n",
    "                    category_to_word_to_count[category][word] = 0;\n",
    "                category_to_word_to_count[category][word] = category_to_word_to_count[category][word] + 1;\n",
    "                word_map[ word ] = True;\n",
    "    \n",
    "    prob_of_word_in_spam_mail = {};\n",
    "    prob_of_word_in_non_spam_mail = {};\n",
    "    vocabsize = len( word_map.keys() )\n",
    "    for word in word_map.keys():\n",
    "        wc_in_spam = 0;\n",
    "        wc_in_non_spam = 0;\n",
    "        if word in category_to_word_to_count[spam].keys():\n",
    "            wc_in_spam = category_to_word_to_count[spam][word];\n",
    "        if word in category_to_word_to_count[notspam].keys():\n",
    "            wc_in_non_spam = category_to_word_to_count[notspam][word];\n",
    "        prob_of_word_in_spam_mail[word] = ( 1 + wc_in_spam )/(wc_in_spam + wc_in_non_spam + vocabsize);\n",
    "        prob_of_word_in_non_spam_mail[word] = ( 1 + wc_in_non_spam)/(wc_in_spam + wc_in_non_spam + vocabsize);\n",
    "    return [ prob_of_word_in_spam_mail, prob_of_word_in_non_spam_mail ]\n",
    "\n",
    "def test( test_data_list, train_metadata ):\n",
    "    [ prob_word_in_spam, prob_word_in_non_spam ] = train_metadata;\n",
    "    testing_accuracy = [ 0.0, 0.0 ];\n",
    "    for test_data in test_data_list:\n",
    "        for data in test_data.keys():\n",
    "            freq_map = mail_content_to_word_freq_map( data );\n",
    "            pm_is_spam = math.log(prob_email_is_spam);\n",
    "            pm_is_non_spam = math.log(prob_email_is_non_spam);\n",
    "            for word in freq_map.keys():\n",
    "                if word in prob_word_in_spam.keys():\n",
    "                    pm_is_spam += math.log( prob_word_in_spam[ word ] );\n",
    "                if word in prob_word_in_non_spam.keys():\n",
    "                    pm_is_non_spam += math.log( prob_word_in_non_spam[ word ] );\n",
    "            \n",
    "            isActuallySpam = test_data[ data ];\n",
    "            if ( pm_is_spam >= pm_is_non_spam ) and isActuallySpam :\n",
    "                    testing_accuracy[0] = testing_accuracy[0] + 1;\n",
    "            elif ( pm_is_spam < pm_is_non_spam ) and ( not isActuallySpam ) :\n",
    "                    testing_accuracy[0] = testing_accuracy[0] + 1;\n",
    "            testing_accuracy[1] = testing_accuracy[1] + 1;\n",
    "    return testing_accuracy[0]/testing_accuracy[1];\n",
    "\n",
    "#P( spam | words in email ) = P( words in email | spam )*P( email is spam ) / P( words in email ) \n",
    "#P( words in email ) = P( words in email | email is spam )*P( email is spam ) + P( words in email | email is not spam )*P( email is not spam)\n",
    "\n",
    "#P( words in email | spam ) = Multiply( for each( P( word in email | spam )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizer True and Stop True -> 0.8434579439252337\n",
      "Lemmatizer True and Stop False -> 0.8422897196261683\n",
      "Lemmatizer False and Stop True -> 0.8422897196261683\n",
      "Lemmatizer False and Stop False -> 0.8422897196261683\n"
     ]
    }
   ],
   "source": [
    "for lemmatizer in [True, False]:\n",
    "    for stop in [True, False]:\n",
    "        datasets = get_datasets( lemmatizer, stop );\n",
    "        train_set = datasets[0:7];\n",
    "        test_set = datasets[7:10];\n",
    "        train_metadata = train( train_set );\n",
    "        test_acc = test( test_set, train_metadata )\n",
    "        print('Lemmatizer {} and Stop {} -> {}'.format(lemmatizer, stop, test_acc));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
